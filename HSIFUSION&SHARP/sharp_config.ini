# SHARP v3.2.2 Training Configuration
# This file contains all configurable parameters for training SHARP models

# ============================================================================
# Model Configuration
# ============================================================================
[model]
# Model size variant: tiny, small, base, large
model_size = base

# Input/output channels
in_channels = 3
out_channels = 31

# ============================================================================
# SHARP v3.2.2 Streaming Sparse Attention Parameters
# ============================================================================
[sparse_attention]
# Sparsity ratio (0.0-1.0): fraction of attention weights to zero out
# 0.9 = keep top 10% of attention weights
sparse_sparsity_ratio = 0.9

# Block size for processing keys in streaming attention
# Larger = more memory but potentially faster
sparse_block_size = 2048

# Maximum sequence length before falling back to local window attention
sparse_max_tokens = 8192

# Window size for local attention fallback (must be odd)
sparse_window_size = 49

# Maximum k values to keep per query (memory cap)
# Set to 0 to disable capping
sparse_k_cap = 1024

# Query block size for memory-efficient tiling
# Smaller = less memory usage
sparse_q_block_size = 1024

# Number of RBF centers per attention head
rbf_centers_per_head = 32

# ============================================================================
# Data Configuration
# ============================================================================
[data]
# Dataset root directory
data_root = ./dataset

# Subdirectories for train/val splits
train_dir = Train_spectral
val_dir = Valid_spectral

# Training data parameters
batch_size = 20
num_workers = 4
patch_size = 128
stride = 8
augment = true

# ============================================================================
# Training Configuration
# ============================================================================
[training]
# Number of training epochs
epochs = 300

# Learning rate
learning_rate = 4e-4

# Weight decay
weight_decay = 1e-4

# Warmup ratio (fraction of total steps)
warmup_ratio = 0.1

# Gradient clipping value
gradient_clip = 1.0

# Exponential moving average decay
ema_decay = 0.999

# Gradient accumulation steps
accumulate_steps = 1

# ============================================================================
# Optimization Settings
# ============================================================================
[optimization]
# Enable automatic mixed precision (AMP)
use_amp = true

# Compile model with torch.compile (requires PyTorch 2.0+)
compile_model = true

# Use channels-last memory format (recommended for convolutions)
use_channels_last = true

# Enable gradient checkpointing (saves memory, slower training)
use_checkpoint = false

# ============================================================================
# Validation Configuration
# ============================================================================
[validation]
# Validation interval (epochs)
val_interval = 10

# Center crop validation images (MST++ protocol)
val_crop = true
val_crop_height = 226
val_crop_width = 256

# ============================================================================
# Checkpointing and Logging
# ============================================================================
[output]
# Output directory for experiments
output_dir = ./experiments/sharp

# Experiment name (auto-generated if not specified)
# Format: sharp_{model_size}_sp{sparsity}_rbf{centers}
experiment_name = 

# Resume training from checkpoint
resume_from = 

# Save checkpoint interval (epochs)
save_interval = 50

# Logging interval (iterations)
log_interval = 100

# ============================================================================
# Hardware Configuration
# ============================================================================
[hardware]
# Device: cuda, cpu
device = cuda

# Random seed (42 for deterministic mode)
seed = 42

# ============================================================================
# Example Configurations for Different Model Sizes
# ============================================================================

# Tiny model (fast training, lower quality)
# model_size = tiny
# sparse_sparsity_ratio = 0.95
# sparse_k_cap = 512
# batch_size = 32

# Small model (balanced)
# model_size = small
# sparse_sparsity_ratio = 0.9
# sparse_k_cap = 768
# batch_size = 24

# Base model (recommended)
# model_size = base
# sparse_sparsity_ratio = 0.9
# sparse_k_cap = 1024
# batch_size = 20

# Large model (best quality, slow training)
# model_size = large
# sparse_sparsity_ratio = 0.85
# sparse_k_cap = 1536
# batch_size = 16

# ============================================================================
# Memory Optimization Tips
# ============================================================================
# If running out of memory:
# 1. Reduce batch_size
# 2. Increase sparse_sparsity_ratio (e.g., to 0.95)
# 3. Reduce sparse_k_cap (e.g., to 512)
# 4. Reduce sparse_block_size and sparse_q_block_size
# 5. Enable use_checkpoint = true
# 6. Increase accumulate_steps (e.g., to 2 or 4)

# ============================================================================
# Performance Optimization Tips
# ============================================================================
# For faster training:
# 1. Enable compile_model = true (requires PyTorch 2.0+)
# 2. Enable use_channels_last = true
# 3. Increase num_workers for data loading
# 4. Use larger sparse_block_size if memory allows
# 5. Disable gradient checkpointing (use_checkpoint = false)

